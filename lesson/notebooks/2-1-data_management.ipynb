{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 2 - Data Management</h1>\n",
    "\n",
    "<font size=\"3\">\n",
    "Goal of this section is to get to know better data management tools, in particular Great Expectation, which allows you to perform data quality assessment & alerting on your day to day projects.\n",
    "\n",
    "We will continue using the TLC trip record data, deep dive specifically into these datasets to catch data quality issues & encode our own set of rules & triggers.\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Getting familiar with the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Downloading our data\n",
    "\n",
    "To start with, let's download our data: we will use a larger dataset containing several details about January 2022 \"for hire vehicles\" trips in NYC (Uber, Lyft...). This dataset has been lightly modified for the purpose of our exercise. Let's download it & save it under our data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "gdown.download(\n",
    "    \"https://drive.google.com/uc?id=1xQ8heQzUkKehOUPYvrHIqQ_pDJNCH9tT\",\n",
    "    \"../data/taxi-trips-2022-01.parquet\",\n",
    "    quiet=False,\n",
    ")\n",
    "gdown.download(\n",
    "    \"https://drive.google.com/uc?id=11kOFkDJIXSW2Hu0o2o-PWBhTJi0msYfH\",\n",
    "    \"../data/taxi-trips-2022-02.parquet\",\n",
    "    quiet=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Loading our data\n",
    "\n",
    "Our data is composed of several columns, the most interesting ones being:\n",
    "- `hvfhs_license_num`: this is the Taxi & License Commission license number of the company operating the trip. Possible values are HV0002 (Juno), HV0003 (Uber), HV0004 (Via), HV0005 (Lyft).\n",
    "- `request_datetime`, `on_scene_datetime`, `pickup_datetime`, `dropoff_datetime`: logs datetime for ride request, when driver arrived, picked-up & dropped off passenger(s).\n",
    "- `PULocationID`, `DOLocationID`: where the trip began & ended. Those are `int` values.\n",
    "- `trip_miles`, `trip_time`: miles for passenger trip & total time in seconds of trip\n",
    "- `base_passenger_fare`: base fare excluding toll (`tolls`), tips (`tips`), taxes (`sales_tax`) and fees (`airport_fee`, `congestion_surcharge`, `bcf`). \n",
    "- `driver_pay`: total driver pay (exclusing tools, tips, commission, taxes...)\n",
    "- `shared_match_flag`: did the passenger share the vehicle with another passenger who booked separately? (Y/N)\n",
    "\n",
    "Let's load it and print the first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_parquet(\"../data/taxi-trips-2022-01.parquet\")\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Exploring our data\n",
    "\n",
    "Let's assume our goal will be to create an application able to predict the fare of a trip, from the pick-up & dropoff locations. We will eventually be using the following columns:\n",
    "- `base_passenger_fare`: our target variable\n",
    "- `hvfhs_license_num`: fare might depend on operating company\n",
    "- `request_datetime`, `on_scene_datetime`, `pickup_datetime`, `dropoff_datetime`: fare might depend on congestion & time of pickup\n",
    "- `PULocationID`, `DOLocationID`: fare will depend on pick up and drop off location\n",
    "- `trip_miles`, `trip_time`: these fields might be useful to normalize training data\n",
    "\n",
    "Let's first explore quality of these key fields. What can you see? Is data quality sufficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** several data quality issues might have a detrimental impact on our model. Which one?\n",
    "- ?\n",
    "\n",
    "We could simple discard these errors, but if we were to continuously train a fare forecasting algorithms, it might be biased or simply break under these data quality issue as soon as a new batch of training data arrives. Let's then put in place some control layers !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Installing Great Expectations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great expectation allows us to:\n",
    "- define data quality rules in a language agnostic format (as config files)\n",
    "- run these data quality checks & rules on various types of data sources\n",
    "- trigger actions & alerting whenever a rule breaks\n",
    "- generate data quality reports easily from our set of rules\n",
    "\n",
    "Your environment should already contain great expectation as a python library. Otherwise you can simply follow the following commands to install it: https://docs.greatexpectations.io/docs/guides/setup/installation/local"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Getting to know Great Expectations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Connecting to our data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see, Great expectations, works with a lot of configuration files (`.yml`, `.json`). This enables us to stay language & datasource agnostic, and to have our rules & checks documented as config and not hard coded.\n",
    "\n",
    "The main entrypoint & best practice to manage 'rules' is to have a folder `great_expectations` where we will store all our config. \n",
    "\n",
    "Before starting implementing checks & triggers, we first need to connect to a dataset, and explain to Great Expectation how to connect to it. This can usually be best done in the following main file: `great_expectations/great_expectations.yml`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "with open(\"../great_expectations/great_expectations.yml\", \"r\") as stream:\n",
    "    try:\n",
    "        ge_config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "pprint(ge_config[\"datasources\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already made part of our task: and told great expectation where to find our dataset, and how to read it (using Pandas & the parquet read function)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Writing a first expectation\n",
    "Data quality rules (or \"expectations\") can also be written in config files and are stored in the `great_expectations/expectations/` folder.\n",
    "We have already written one expecting the base fare not to be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../great_expectations/expectations/taxi-trips-expectations.json\") as f:\n",
    "    expectation = json.load(f)\n",
    "\n",
    "pprint(expectation[\"expectations\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Checking our data\n",
    "Now that we can connect to our data... and have defined a set of data quality rules, how do we apply these rules to our datasources? As you would expect, great expectations also uses configuration files to run data checks, as found in the `great_expectations/checkpoints/` folder. Where we bin a datasource (and particularly a data asset) to a suite of expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../great_expectations/checkpoints/taxi-trips-checkpoint.yml\", \"r\") as stream:\n",
    "    try:\n",
    "        chkp_config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "pprint(chkp_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running our checkpoint, let's introduce the `great_expectations.data_context`: this object scans your repository and stores all datasources, checkpoints & expectations you have defined. You can then handle them from your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "import great_expectations.jupyter_ux\n",
    "from great_expectations.datasource.types import BatchKwargs\n",
    "import datetime\n",
    "\n",
    "context = ge.data_context.DataContext()\n",
    "print(context.list_expectation_suite_names())\n",
    "print([datasource[\"name\"] for datasource in context.list_datasources()])\n",
    "print(context.list_checkpoints())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now run a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run_checkpoint(checkpoint_name=\"taxi-trips-checkpoint\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that GE allows you to export your results in a simple html format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.open_data_docs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - More expectations & more data!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. More expectations\n",
    "Now use what you have learnt to great 2 or 3 more expectations for your data. You can look for ideas there: https://greatexpectations.io/expectations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../great_expectations/expectations/taxi-trips-expectations-solution.json\") as f:\n",
    "    expectation = json.load(f)\n",
    "\n",
    "pprint(expectation[\"expectations\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. More data assets\n",
    "Before running our checkpoint, change the `great_expectations.yml` file so that it now catches data from february (`data/taxi-trips-2022-02.parquet`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../great_expectations/great_expectations-solution.yml\", \"r\") as stream:\n",
    "    try:\n",
    "        ge_config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "pprint(ge_config[\"datasources\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Running our new expectations\n",
    "Update your checkpoint file & run the expectations you have just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Wrapping up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this short tutorial, you have seen how to configure a simple great expectations project & run a few data quality rules. The main takeaway is that GE allows you to create expectations & run them entirely with configuration, abstracting the connection to data sources behind.\n",
    "\n",
    "Other exercices you could work on:\n",
    "- Connecting to a distant datasource (s3, BigQuery...)\n",
    "- Writing your own expectation (not available in the gallery)\n",
    "- Using great expectations actions to avoid deploying if data quality is not as expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
